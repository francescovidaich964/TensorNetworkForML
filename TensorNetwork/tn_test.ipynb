{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tnrange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,\"../\")\n",
    "import data_generator as gen\n",
    "import TensorNetwork as tn\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Add description\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index], self.label[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data, label, train_perc, val_perc, train_batch_size, val_batch_size, test_batch_size):\n",
    "    \"\"\"\n",
    "    Add description\n",
    "    \"\"\"\n",
    "    \n",
    "    def psi(x):\n",
    "        x = np.array((np.sin(np.pi*x/2),np.cos(np.pi*x/2)))\n",
    "        return np.transpose(x, [1,2,0])\n",
    "\n",
    "\n",
    "    # flatten images\n",
    "    x = data.reshape(len(data),-1)\n",
    "    # embedd them\n",
    "    x = psi(x)\n",
    "    \n",
    "    # training/test splitting\n",
    "    m = int(len(x)*train_perc)\n",
    "    x_train= x[:m]\n",
    "    y_train = label[:m]\n",
    "    x_test =  x[m:]\n",
    "    y_test = label[m:]\n",
    "    \n",
    "    # define custom NumpyDatasets\n",
    "    train_set = NumpyDataset(x_train, y_train)\n",
    "    test_set =  NumpyDataset(x_test, y_test)\n",
    "   \n",
    "    train_len = int(m*(1-val_perc))\n",
    "    train_sampler = SubsetRandomSampler(np.arange(train_len))\n",
    "    val_sampler = SubsetRandomSampler(np.arange(train_len,m))\n",
    "\n",
    "    train_loader = DataLoader(train_set, train_batch_size, sampler=train_sampler, drop_last=True, collate_fn=lambda x: x)\n",
    "    val_loader = DataLoader(train_set, val_batch_size, sampler=val_sampler, drop_last=True, collate_fn=lambda x: x)\n",
    "    test_loader = DataLoader(test_set, test_batch_size, drop_last=False, collate_fn=lambda x: x)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5000\n",
    "linear_dim = 14\n",
    "M = 20\n",
    "B = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data, label) = gen.create_dataset(n_samples, linear_dim=linear_dim, sigma=0.2)\n",
    "batch_size = {'train_batch_size':B, 'val_batch_size':128, 'test_batch_size':128}\n",
    "train_loader, val_loader, test_loader = prepare_dataset(data, label, 0.8, 0.2, **batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'TensorNetwork' from '/home/francesco/UNIPD/Information_Theory/TensorNetworkForML/TensorNetwork/TensorNetwork.py'>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_batch = next(iter(train_loader))\n",
    "x_calibration = np.array([calibration_batch[i][0] for i in range(len(calibration_batch))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing weights...\n",
      "Scaling factor: 12.80\n",
      "(32, 196, 2)\n",
      "f_max for random input of 32 samples :  5.442124936715372e-10\n",
      "F2:  0.8968782926113396\n",
      "Sum of all elements of all As (after init):  6878.355616689671\n",
      "f_max for random input of 32 samples (after):  1.0000000000000016\n"
     ]
    }
   ],
   "source": [
    "N = linear_dim**2\n",
    "net = tn.Network(N=N, M=M, L=2, normalize=True, calibration_X=x_calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52594112 0.31949251 0.17631667 0.16026242 0.15371059 0.25920039\n",
      "  0.36813124 0.10097309 0.15001716 0.19104487 0.66661441 0.04460975\n",
      "  0.15266777 0.23626935 0.17200058 0.05339378 0.75180778 0.11090527\n",
      "  0.16773258 0.50599904 0.56505869 0.16918613 0.07927358 0.0908635\n",
      "  0.15950134 0.096605   0.62605192 0.10068467 0.48199532 1.\n",
      "  0.31028337 0.32326158]\n",
      " [0.50664521 0.32169029 0.17098757 0.15437501 0.1546114  0.26145547\n",
      "  0.37107765 0.09734777 0.14976592 0.19345229 0.64141353 0.04461563\n",
      "  0.15414347 0.2269472  0.17346364 0.05158346 0.72678062 0.10727644\n",
      "  0.16727555 0.50947846 0.56662513 0.16259426 0.07633887 0.09115195\n",
      "  0.1603377  0.0962459  0.6059402  0.0979532  0.48800336 0.96061951\n",
      "  0.30011385 0.3121011 ]]\n"
     ]
    }
   ],
   "source": [
    "f = net.forward(x_calibration)\n",
    "print(f.elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8836ee0e85ef459f9ce84bd785a1066e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch loop', max=10, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.60298352711727\n",
      "13.378610747770336\n",
      "12.731484714098894\n",
      "12.572135118992819\n",
      "12.849278569672824\n",
      "13.425322499845304\n",
      "12.361997256081613\n",
      "13.810012075478816\n",
      "13.368150897530251\n",
      "14.161176045266913\n",
      "14.60169140267236\n",
      "13.52669825196112\n",
      "13.113897349215032\n",
      "13.545746008512177\n",
      "12.414170389405825\n",
      "13.175062899074806\n",
      "13.41205870439936\n",
      "13.312304748940635\n",
      "14.208960857603696\n",
      "13.40841049459378\n",
      "11.64356503821319\n",
      "13.117184980952038\n",
      "14.097317458040319\n",
      "14.61043662449717\n",
      "13.379566314982963\n",
      "14.93180518275674\n",
      "14.943456977263544\n",
      "12.241294144290876\n",
      "15.137404220584289\n",
      "14.044905853763339\n",
      "13.186980159895173\n",
      "14.019471391456618\n",
      "11.615029266738201\n",
      "12.431738556064788\n",
      "10.087547717293463\n",
      "10.203062572834995\n",
      "8.972707015287249\n",
      "10.888240770092942\n",
      "10.142146683539636\n",
      "9.439192295421265\n",
      "8.891433462068534\n",
      "11.936324751119706\n",
      "11.08814110969351\n",
      "9.761365551601584\n",
      "9.632978869448143\n",
      "12.368046715122146\n",
      "11.332110043123729\n",
      "10.984310315670097\n",
      "11.17533107054113\n",
      "11.361485780556295\n",
      "12.761478386450658\n",
      "12.23054566155466\n",
      "10.223927858741472\n",
      "11.586137654600542\n",
      "11.043202551923486\n",
      "12.170990411104745\n",
      "11.544731558356341\n",
      "9.576398410869018\n",
      "8.752890859960834\n",
      "6.8431706793512035\n",
      "3.2798492262695067\n",
      "0.9051766218671853\n",
      "0.17917852099801757\n",
      "0.1720765429643179\n",
      "0.1571837019211308\n",
      "0.15077285126773127\n",
      "0.15473953792683004\n",
      "0.13275453651561236\n",
      "0.1269036140580342\n",
      "0.11964034181663546\n",
      "0.11054655845032507\n",
      "0.10132944407560054\n",
      "0.09249628548649828\n",
      "0.08304185279785212\n",
      "0.06927538736021052\n",
      "0.06874861998553344\n",
      "0.06143015330436726\n",
      "0.0508870345431353\n",
      "0.04652890478142886\n",
      "0.043875227377280335\n",
      "0.03709278525930318\n",
      "0.03549102553646836\n",
      "0.03145703425553427\n",
      "0.032308827998442674\n",
      "0.027560784793513118\n",
      "0.024023150518688217\n",
      "0.01833772192293579\n",
      "0.015441932769706092\n",
      "0.01604813566464073\n",
      "0.016851454269123947\n",
      "0.016862403003339323\n",
      "0.015022450733223645\n",
      "0.013314720597522218\n",
      "0.011341991495445118\n",
      "0.010598782566814385\n",
      "0.010170190042349508\n",
      "0.008981928832408706\n",
      "0.007450118272415743\n",
      "0.007554258026087088\n",
      "0.006941418065320322\n",
      "0.0067557683879886764\n",
      "0.006167060904879434\n",
      "0.005780529039855117\n",
      "0.005394935555033082\n",
      "0.00503583121446294\n",
      "0.00449589260233535\n",
      "0.003858622098545205\n",
      "0.00297459912640985\n",
      "0.0022597365714583364\n",
      "0.002027067451655248\n",
      "0.0019423184196105552\n",
      "0.0018415535583890669\n",
      "0.0017145409214054399\n",
      "0.0017697816264227776\n",
      "0.0015796152475676558\n",
      "0.001406545903882196\n",
      "0.001270998523520047\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-b7e63445f58e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/UNIPD/Information_Theory/TensorNetworkForML/TensorNetwork/TensorNetwork.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, val_loader, lr, n_epochs, early_stopping, print_freq)\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m                 \u001b[0mbatch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msweep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m                 \u001b[0mepoch_train_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/UNIPD/Information_Theory/TensorNetworkForML/TensorNetwork/TensorNetwork.py\u001b[0m in \u001b[0;36msweep\u001b[0;34m(self, X, y, f, lr)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0;31m#print(\"\\nright sweep step \",i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_sweep_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m             \u001b[0;31m####################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m#sum_of_A = 0 # debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/UNIPD/Information_Theory/TensorNetworkForML/TensorNetwork/TensorNetwork.py\u001b[0m in \u001b[0;36mr_sweep_step\u001b[0;34m(self, f, y, lr, batch_size)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;31m# just trying to regularize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0;31m# B.elem *= (1-lr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdeltaB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'B.elem.sum() (after update): '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/UNIPD/Information_Theory/TensorNetworkForML/TensorNetwork/TensorNetwork.py\u001b[0m in \u001b[0;36m__add__\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# check all names match between two tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Error: axes don't match, cannot sum tensors.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36misin\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36misin\u001b[0;34m(element, test_elements, assume_unique, invert)\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     return in1d(element, test_elements, assume_unique=assume_unique,\n\u001b[0;32m--> 696\u001b[0;31m                 invert=invert).reshape(element.shape)\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36min1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36min1d\u001b[0;34m(ar1, ar2, assume_unique, invert)\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mar2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m                 \u001b[0mmask\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_acc, val_acc = net.train(train_loader, val_loader, lr = 0.01, n_epochs = 10, print_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_loader))\n",
    "x = np.array([data[i][0] for i in range(len(data))])\n",
    "y = np.array([data[i][1] for i in range(len(data))])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = net.forward(x)\n",
    "print(f.elem.shape)\n",
    "y_pred = np.argmax(f.elem, axis=0)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.accuracy(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling in the number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = [1000*i**2 for i in range(1,9)]\n",
    "print(\"Number of samples tried: \\n\", n_samples)\n",
    "\n",
    "# keep them fixed\n",
    "linear_dim = 5\n",
    "M = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "times_n_samples = []\n",
    "for n in n_samples:\n",
    "    print(\"Testing with %d samples...\"%n)\n",
    "    (data, label) = gen.create_dataset(n, sigma=0.7)\n",
    "    batch_size = {'train_batch_size':16, 'val_batch_size':128, 'test_batch_size':128}\n",
    "    train_loader, val_loader, test_loader = prepare_dataset(data, label, 0.8, 0.2, **batch_size)\n",
    "    \n",
    "    # independent from the number of samples\n",
    "    net = tn.Network(N=linear_dim**2, M=M, L=2)\n",
    "    \n",
    "    start = time.time()\n",
    "    train_acc, val_acc = net.train(train_loader, val_loader, lr = 0.5, n_epochs = 1, print_freq=1)\n",
    "    dt = time.time() - start\n",
    "    print(\"Time for 1 epoch: %.2f s \\n\"%dt)\n",
    "    times_n_samples.append(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(n_samples, times_n_samples, '-o', label='M = %d \\nlinear dim = %d'%(M,linear_dim))\n",
    "plt.xlabel(\"Number of samples\", fontsize=16)\n",
    "plt.ylabel(\"Time for one epoch [s]\", fontsize=16)\n",
    "plt.title(r\"Training time vs $n_{samples}$\", fontsize=16)\n",
    "plt.legend(fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling in the number of pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_dims = np.arange(4,29,4)\n",
    "print(\"Linear dimensions tested: \\n\", linear_dims)\n",
    "\n",
    "# keep them fixed\n",
    "n_samples = 1000\n",
    "M = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "times_linear_dims = []\n",
    "for linear_dim in linear_dims:\n",
    "    print(\"Testing with %d x %d images...\"%(linear_dim,linear_dim))\n",
    "    (data, label) = gen.create_dataset(n_samples, sigma=0.7, linear_dim = linear_dim)\n",
    "    batch_size = {'train_batch_size':16, 'val_batch_size':128, 'test_batch_size':128}\n",
    "    train_loader, val_loader, test_loader = prepare_dataset(data, label, 0.8, 0.2, **batch_size)\n",
    "    \n",
    "    # independent from the number of samples\n",
    "    net = tn.Network(N=linear_dim**2, M=M, L=2)\n",
    "    \n",
    "    start = time.time()\n",
    "    train_acc, val_acc = net.train(train_loader, val_loader, lr = 0.5, n_epochs = 1, print_freq=1)\n",
    "    dt = time.time() - start\n",
    "    print(\"Time for 1 epoch: %.2f s \\n\"%dt)\n",
    "    times_linear_dims.append(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(linear_dims**2, times_linear_dims, '-o', label='M = %d \\nnum samples = %d'%(M,n_samples))\n",
    "plt.xlabel(\"Number of pixels\", fontsize=16)\n",
    "plt.ylabel(\"Time for one epoch [s]\", fontsize=16)\n",
    "plt.title(r\"Training time vs $n_{pixels}$\", fontsize=16)\n",
    "plt.legend(fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling in the bond dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ms = [10*i for i in range(1,11)]\n",
    "print(\"Bond dimensions tested: \\n\", Ms)\n",
    "\n",
    "# keep them fixed\n",
    "n_samples = 1000\n",
    "linear_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "times_bond_dims = []\n",
    "for M in Ms:\n",
    "    print(\"Testing with bond dimension %d...\"%M)\n",
    "    (data, label) = gen.create_dataset(n_samples, sigma=0.7)\n",
    "    batch_size = {'train_batch_size':16, 'val_batch_size':128, 'test_batch_size':128}\n",
    "    train_loader, val_loader, test_loader = prepare_dataset(data, label, 0.8, 0.2, **batch_size)\n",
    "    \n",
    "    # independent from the number of samples\n",
    "    net = tn.Network(N=linear_dim**2, M=M, L=2)\n",
    "    \n",
    "    start = time.time()\n",
    "    train_acc, val_acc = net.train(train_loader, val_loader, lr = 0.5, n_epochs = 1, print_freq=1)\n",
    "    dt = time.time() - start\n",
    "    print(\"Time for 1 epoch: %.2f s \\n\"%dt)\n",
    "    times_bond_dims.append(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(Ms, times_bond_dims, '-o', label='linear dim = %d \\nnum samples = %d'%(linear_dim,n_samples))\n",
    "plt.xlabel(\"Bond dimension\", fontsize=16)\n",
    "plt.ylabel(\"Time for one epoch [s]\", fontsize=16)\n",
    "plt.title(\"Training time vs bond dimension\", fontsize=16)\n",
    "plt.legend(fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight initialization\n",
    "\n",
    "**Objective:** study what is the dependence in the output magnitude of the various dimensions so that we can choose how to normalize the matrices A during initialization.\n",
    "\n",
    "**Parameters:**\n",
    "* Number of pixels N\n",
    "* Bond dimension M\n",
    "* Batch size B\n",
    "* Embedding dimension D (fixed to 2)\n",
    "\n",
    "Only the first 3 will be inquired.\n",
    "\n",
    "**Methodology:** Use random input in [0,1] and random initialization in [0,1] for different dimensions and look at the absolute value of the output (take the output among the L with the highest absolute value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependence on the number of pixels N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_dims = np.arange(4,16)\n",
    "print(\"Linear dimensions tested: \\n\", linear_dims)\n",
    "\n",
    "# keep them fixed\n",
    "n_samples = 1000\n",
    "M = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_linear_dims = []\n",
    "for linear_dim in linear_dims:\n",
    "    print(\"Testing with %d x %d images...\"%(linear_dim,linear_dim))\n",
    "    (data, label) = gen.create_dataset(n_samples, sigma=0.7, linear_dim = linear_dim)\n",
    "    batch_size = {'train_batch_size':16, 'val_batch_size':128, 'test_batch_size':128}\n",
    "    train_loader, val_loader, test_loader = prepare_dataset(data, label, 0.8, 0.2, **batch_size)\n",
    "    \n",
    "    # independent from the number of samples\n",
    "    net = tn.Network(N=linear_dim**2, M=M, L=2)\n",
    "    \n",
    "    batch = next(iter(train_loader))\n",
    "    X = np.array([batch[i][0] for i in range(len(batch))])\n",
    "    f = net.forward(X)\n",
    "    f_max = np.abs(f.elem).max()\n",
    "    magnitude_linear_dims.append(f_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_linear_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = linear_dims**2\n",
    "y = np.log10(magnitude_linear_dims)\n",
    "X = Ns.reshape(-1,1)\n",
    "reg = LinearRegression().fit(X, y)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(Ns, y, label = 'data')\n",
    "plt.plot(Ns, reg.predict(X), label = 'slope = %.2f'%reg.coef_)\n",
    "plt.ylabel(r'$log_{10}(f_{max})$', fontsize = 16)\n",
    "plt.xlabel('Number of pixels N', fontsize = 16)\n",
    "plt.legend(fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the dependence on N is exponential, meaning that we can have an exponential suppression (or divergence) if the network is not accurately initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolating dependence C(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaled = y - Ns*np.log10(M)\n",
    "X = Ns.reshape(-1,1)\n",
    "reg_C = LinearRegression().fit(X, y_scaled)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(Ns, y_scaled, label = 'data')\n",
    "plt.plot(Ns, reg_C.predict(X), label = 'slope = %.2f'%reg_C.coef_)\n",
    "plt.ylabel(r'$log_{10}(f_{max})$', fontsize = 16)\n",
    "plt.xlabel('Number of pixels N', fontsize = 16)\n",
    "plt.legend(fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/(10**(-0.18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bond dimension M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ms = [5*i for i in range(1,21)]\n",
    "print(\"Bond dimensions tested: \\n\", Ms)\n",
    "\n",
    "# keep them fixed\n",
    "n_samples = 1000\n",
    "linear_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "magnitude_bond_dims = []\n",
    "for M in Ms:\n",
    "    print(\"Testing with bond dimension %d...\"%M)\n",
    "    (data, label) = gen.create_dataset(n_samples, sigma=0.7, linear_dim=linear_dim)\n",
    "    batch_size = {'train_batch_size':16, 'val_batch_size':128, 'test_batch_size':128}\n",
    "    train_loader, val_loader, test_loader = prepare_dataset(data, label, 0.8, 0.2, **batch_size)\n",
    "    \n",
    "    # independent from the number of samples\n",
    "    net = tn.Network(N=linear_dim**2, M=M, L=2)\n",
    "    \n",
    "    batch = next(iter(train_loader))\n",
    "    X = np.array([batch[i][0] for i in range(len(batch))])\n",
    "    f = net.forward(X)\n",
    "    f_max = np.abs(f.elem).max()\n",
    "    magnitude_bond_dims.append(f_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log10(magnitude_bond_dims)\n",
    "X = np.log10(np.array(Ms)).reshape(-1,1)\n",
    "reg_M = LinearRegression().fit(X, y)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(X, y, label = 'data')\n",
    "plt.plot(X, reg_M.predict(X), label = 'slope = %.3f'%reg_M.coef_)\n",
    "plt.ylabel(r'$log_{10}(f_{max})$', fontsize = 16)\n",
    "plt.xlabel(r'Bond dimension $log_{10}(M)$', fontsize = 16)\n",
    "plt.legend(fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(10**reg_M.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output magnitude has a power law dependence on the bond dimension. Notice how the slope is equal to N.\n",
    "This makes sense since we already know that f is exponential in N. Now we also can approximate the scaling of the f as:\n",
    "$$f \\approx C(N)*M^N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch dimension B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 20\n",
    "n_samples = 1000\n",
    "linear_dim = 5\n",
    "\n",
    "train_batch_sizes = np.array([2**i for i in range(1,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_batch_size = []\n",
    "for B in train_batch_sizes:\n",
    "    (data, label) = gen.create_dataset(n_samples, sigma=0.7, linear_dim = linear_dim)\n",
    "    batch_size = {'train_batch_size': int(B), 'val_batch_size':128, 'test_batch_size':128}\n",
    "    train_loader, val_loader, test_loader = prepare_dataset(data, label, 0.8, 0.2, **batch_size)\n",
    "    \n",
    "    # independent from the number of samples\n",
    "    net = tn.Network(N=linear_dim**2, M=M, L=2)\n",
    "    \n",
    "    batch = next(iter(train_loader))\n",
    "    X = np.array([batch[i][0] for i in range(len(batch))])\n",
    "    f = net.forward(X)\n",
    "    f_max = np.abs(f.elem).max()\n",
    "    magnitude_batch_size.append(f_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(train_batch_sizes, magnitude_batch_size, label = 'data')\n",
    "plt.ylabel(r'$f_{max}$', fontsize = 16)\n",
    "plt.xlabel('Batch size B', fontsize = 16)\n",
    "plt.legend(fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No clear dependence emerges on the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion on initialization\n",
    "\n",
    "The simplest and most reliable initialization is the one that takes into account the number of pixels given as input and disregards all the rest. However remains the problem of choosing the factor with which the weights should be divided. To find an exact factor a priori is not immediate and is also a risky procedure, hence it's better to devise an empirical method to do so.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_dim = 5\n",
    "n_samples = 1000\n",
    "M = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Cs = np.arange(1,21)\n",
    "(data, label) = gen.create_dataset(n_samples, sigma=0.7, linear_dim = linear_dim)\n",
    "batch_size = {'train_batch_size':16, 'val_batch_size':128, 'test_batch_size':128}\n",
    "train_loader, val_loader, test_loader = prepare_dataset(data, label, 0.8, 0.2, **batch_size)\n",
    "batch = next(iter(train_loader))\n",
    "X = np.array([batch[i][0] for i in range(len(batch))])\n",
    "\n",
    "magnitude_C = []\n",
    "for C in Cs:\n",
    "    \n",
    "    net = tn.Network(N=linear_dim**2, M=M, L=2, normalize=False)\n",
    "    \n",
    "    for i in range(net.N):\n",
    "        net.As[i].elem *= C\n",
    "    \n",
    "    f = net.forward(X)\n",
    "    f_max = np.abs(f.elem).max()\n",
    "    print('f_max: ', f_max)\n",
    "    magnitude_C.append(f_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "X = np.log10(Cs).reshape(-1,1)\n",
    "y = np.log10(np.array(magnitude_C))\n",
    "y = y/y[0]\n",
    "reg_scale = LinearRegression().fit(X, y)\n",
    "plt.plot(X, y, label='data')\n",
    "plt.plot(X, reg_scale.predict(X), label = 'slope = %.2f\\nintercept = %.2f'%(reg_scale.coef_,reg_scale.intercept_))\n",
    "plt.ylabel(r'$log_{10}(f_{max})$', fontsize = 16)\n",
    "plt.xlabel(r'Scaling factor $log_{10}(C)$', fontsize = 16)\n",
    "plt.legend(fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ms = np.array([5*i for i in range(1,21)])\n",
    "Cs = np.arange(1,21)\n",
    "(data, label) = gen.create_dataset(n_samples, sigma=0.7, linear_dim = linear_dim)\n",
    "batch_size = {'train_batch_size':16, 'val_batch_size':128, 'test_batch_size':128}\n",
    "train_loader, val_loader, test_loader = prepare_dataset(data, label, 0.8, 0.2, **batch_size)\n",
    "batch = next(iter(train_loader))\n",
    "X = np.array([batch[i][0] for i in range(len(batch))])\n",
    "\n",
    "magnitude_C_M = []\n",
    "for M in Ms:\n",
    "    magnitude_C = []\n",
    "    for C in Cs:\n",
    "\n",
    "        net = tn.Network(N=linear_dim**2, M=M, L=2, normalize=False)\n",
    "\n",
    "        for i in range(net.N):\n",
    "            net.As[i].elem *= C\n",
    "\n",
    "        f = net.forward(X)\n",
    "        f_max = np.abs(f.elem).max()\n",
    "        magnitude_C.append(f_max)\n",
    "        \n",
    "    magnitude_C = np.array(magnitude_C)\n",
    "    magnitude_C_M.append(magnitude_C)\n",
    "    \n",
    "magnitude_C_M = np.array(magnitude_C_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_C_M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = []\n",
    "for i,M in enumerate(Ms):\n",
    "    X = np.log10(Cs).reshape(-1,1)\n",
    "    y = np.log10(np.array(magnitude_C_M[i,:]))\n",
    "    y = y/y[0]\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "    alphas.append(reg.coef_)\n",
    "alphas = np.array(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(Ms, alphas)\n",
    "plt.xlabel('Bond dimension M', fontsize=16)\n",
    "plt.ylabel(r'$\\alpha$ coefficient', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_dims = np.arange(4,29,4)\n",
    "print(\"Linear dimensions tested: \\n\", linear_dims)\n",
    "\n",
    "# keep them fixed\n",
    "n_samples = 1000\n",
    "M = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_linear_dims = []\n",
    "for linear_dim in linear_dims:\n",
    "    print(\"Testing with %d x %d images...\"%(linear_dim,linear_dim))\n",
    "    (data, label) = gen.create_dataset(n_samples, sigma=0.7, linear_dim = linear_dim)\n",
    "    batch_size = {'train_batch_size':16, 'val_batch_size':128, 'test_batch_size':128}\n",
    "    train_loader, val_loader, test_loader = prepare_dataset(data, label, 0.8, 0.2, **batch_size)\n",
    "    \n",
    "    # independent from the number of samples\n",
    "    net = tn.Network(N=linear_dim**2, M=M, L=2, normalize=True)\n",
    "    \n",
    "    batch = next(iter(train_loader))\n",
    "    X = np.array([batch[i][0] for i in range(len(batch))])\n",
    "    f = net.forward(X)\n",
    "    f_max = np.abs(f.elem).max()\n",
    "    magnitude_linear_dims.append(f_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.random.random(10000)\n",
    "x = np.cos(np.pi/2*u)\n",
    "x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_linear_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = linear_dims**2\n",
    "y = np.log10(magnitude_linear_dims)\n",
    "X = Ns.reshape(-1,1)\n",
    "reg = LinearRegression().fit(X, y)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(Ns, y, label = 'data')\n",
    "plt.plot(Ns, reg.predict(X), label = 'slope = %.2f'%reg.coef_)\n",
    "plt.ylabel(r'$log_{10}(f_{max})$', fontsize = 16)\n",
    "plt.xlabel('Number of pixels N', fontsize = 16)\n",
    "plt.legend(fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient clipping\n",
    "**Idea**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.measure\n",
    "\n",
    "def pooling(X):\n",
    "    X = skimage.measure.block_reduce(X, (1,2,2), np.max)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_data = gen.get_MNIST_dataset(data_root_dir = './datasets', download=False)\n",
    "train_data, train_labels, test_data, test_labels = MNIST_data\n",
    "data = np.concatenate((train_data,test_data))\n",
    "data = pooling(data)\n",
    "labels = np.concatenate((train_labels,test_labels))\n",
    "batch_size = {'train_batch_size':32, 'val_batch_size':128, 'test_batch_size':128}\n",
    "train_loader, val_loader, test_loader = prepare_dataset(data, labels, 6/7, 0.2, **batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = (labels == 0) \n",
    "mask2 = (labels == 1)\n",
    "mask = mask1 + mask2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels01 = labels[mask]\n",
    "data01 = data[mask]\n",
    "batch_size = {'train_batch_size':128, 'val_batch_size':128, 'test_batch_size':128}\n",
    "train_loader, val_loader, test_loader = prepare_dataset(data01, labels01, 6/7, 0.2, **batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'TensorNetwork' (namespace)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_batch = next(iter(train_loader))\n",
    "x_calibration = np.array([calibration_batch[i][0] for i in range(len(calibration_batch))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'TensorNetwork' has no attribute 'Network'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e829b7745ce3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m196\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcalibration_X\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_calibration\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, sigma=0.23)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'TensorNetwork' has no attribute 'Network'"
     ]
    }
   ],
   "source": [
    "net = tn.Network(N=196, M=20, L=2, normalize=True,calibration_X=x_calibration) #, sigma=0.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_acc, val_acc = net.train(train_loader, val_loader, lr = 0.01, n_epochs = 10, print_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 5\n",
    "plt.imshow(data01[n])\n",
    "plt.show()\n",
    "labels01[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
